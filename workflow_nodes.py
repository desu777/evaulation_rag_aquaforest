# workflow_nodes.py - Core Workflow Nodes
from datetime import datetime
from langchain_core.messages import SystemMessage
from state import EnhancedEvaluationRAGState
from components import EvaluationRAGComponents
from config import ENHANCED_AQUAFOREST_EXPERT_PROMPT, MAX_REASONING_ATTEMPTS

# Initialize components
components = EvaluationRAGComponents()

def initialize_evaluation(state: EnhancedEvaluationRAGState) -> EnhancedEvaluationRAGState:
    """Node 1: Initialize evaluation-based reasoning"""
    print(f"ğŸ§  ENHANCED EVALUATION RAG: {state['original_query']}")
    print("ğŸ¯ Model-based quality assessment + Intent detection")
    
    return {
        **state,
        "current_query": state['original_query'],
        "attempt_count": 0,
        "attempt_history": [],
        "all_results": [],
        "evaluation_log": [f"Starting evaluation for: '{state['original_query']}'"],
        "should_continue": True,
        "escalate": False,
        "query_intent": "general",
        "business_type": "none",
        "requires_trade_secret_filter": False,
        "confidence_threshold_override": 7.0,
        "company_context_added": False
    }

def execute_search_attempt(state: EnhancedEvaluationRAGState) -> EnhancedEvaluationRAGState:
    """Node 2: Execute search with improved query optimization"""
    attempt = state["attempt_count"] + 1
    print(f"\nğŸ” ATTEMPT {attempt}/3")
    
    # Get previous evaluations for context
    previous_evaluations = state["evaluation_log"]
    
    # Optimize query for this attempt - MUCH simpler now
    optimized_query = components.optimize_query_for_attempt(
        state['original_query'], 
        attempt, 
        previous_evaluations
    )
    
    print(f"ğŸ”§ Query: {optimized_query}")
    
    # Search with optimized query (no score filtering!)
    results = components.search_knowledge(optimized_query)
    
    print(f"ğŸ“Š Found {len(results)} results")
    
    # Log attempt details
    attempt_info = {
        "attempt": attempt,
        "query": optimized_query,
        "results_count": len(results),
        "timestamp": datetime.now().isoformat()
    }
    
    evaluation_entry = f"Attempt {attempt}: '{optimized_query}' â†’ {len(results)} results found"
    
    return {
        **state,
        "current_query": optimized_query,
        "attempt_count": attempt,
        "search_results": results,
        "attempt_history": [attempt_info],
        "all_results": [results],
        "evaluation_log": [evaluation_entry]
    }

def enhanced_evaluate_content_quality(state: EnhancedEvaluationRAGState) -> EnhancedEvaluationRAGState:
    """Node 3: Enhanced evaluation with smart fallbacks"""
    attempt = state["attempt_count"]
    results = state["search_results"]
    
    print(f"ğŸ¤– Model evaluating content quality...")
    
    # Get dynamic threshold based on intent
    confidence_threshold = state.get("confidence_threshold_override", 7.0)
    
    # Model evaluates content (ignoring Pinecone scores)
    confidence, reasoning = components.evaluate_content_quality(
        state["original_query"], 
        results
    )
    
    print(f"ğŸ“Š Model confidence: {confidence}/10 (threshold: {confidence_threshold})")
    print(f"ğŸ’­ Reasoning: {reasoning[:100]}...")
    
    evaluation_entry = f"Model evaluation: {confidence}/10 - {reasoning[:50]}..."
    
    # Dosage-specific fallback (instead of escalation!)
    if (state["query_intent"] == "dosage" and 
        confidence < confidence_threshold and 
        attempt >= MAX_REASONING_ATTEMPTS):
        
        print(f"ğŸ’Š DOSAGE FALLBACK: Providing packaging instructions")
        
        dosage_fallback = f"""Nie znalazÅ‚em szczegÃ³Å‚owych informacji o dawkowaniu w bazie wiedzy Aquaforest.

ğŸ“¦ **Instrukcje dawkowania znajdujÄ… siÄ™ na opakowaniu produktu**
- Zawsze sprawdÅº etykietÄ™ przed uÅ¼yciem
- Rozpocznij od najmniejszej zalecanej dawki  
- Obserwuj reakcjÄ™ akwarium i dostosuj dawkÄ™

ğŸ’¡ **OgÃ³lne zasady Aquaforest:**
- Dawki na etykiecie sÄ… bezpieczne dla standardowych akwariÃ³w
- W przypadku wÄ…tpliwoÅ›ci skonsultuj z doÅ›wiadczonym akwarystÄ…
- Regularnie testuj parametry wody po dodaniu preparatu

ğŸ“ **Pomoc techniczna**: (+48) 14 691 79 79 (pon-pt, 8:00-16:00)

ğŸŒ **WiÄ™cej informacji**: https://aquaforest.eu/pl/kontakt/"""
        
        return {
            **state, 
            "final_answer": dosage_fallback,
            "should_continue": False,
            "model_confidence": 7.0,  # Good fallback response
            "evaluation_log": [evaluation_entry + " | Dosage fallback used"]
        }
    
    # Standard confidence evaluation
    if confidence >= confidence_threshold:
        status = "EXCELLENT" if confidence >= 9.0 else "GOOD"
        print(f"âœ… {status} content quality ({confidence}/10) - generating answer")
        return {
            **state, 
            "model_confidence": confidence,
            "should_continue": False,
            "evaluation_log": [evaluation_entry]
        }
        
    elif attempt >= MAX_REASONING_ATTEMPTS:
        print(f"âš ï¸ Max attempts reached ({attempt}) - escalating")
        evaluation_entry += f" | Max attempts reached"
        return {
            **state, 
            "model_confidence": confidence,
            "should_continue": False, 
            "escalate": True,
            "evaluation_log": [evaluation_entry]
        }
        
    else:
        print(f"ğŸ¤” Content quality insufficient ({confidence}/10) - continuing reasoning")
        evaluation_entry += f" | Insufficient quality, continuing"
        return {
            **state, 
            "model_confidence": confidence,
            "should_continue": True,
            "evaluation_log": [evaluation_entry]
        }

def generate_evaluation_answer(state: EnhancedEvaluationRAGState) -> EnhancedEvaluationRAGState:
    """Node 4: Generate final answer based on evaluated content"""
    
    if state.get("escalate"):
        escalation_response = """Przepraszam, nie udaÅ‚o mi siÄ™ znaleÅºÄ‡ wystarczajÄ…co precyzyjnych informacji na Twoje pytanie w bazie wiedzy Aquaforest.

ğŸ” **MoÅ¼esz sprÃ³bowaÄ‡:**
1. ZadaÄ‡ pytanie bardziej szczegÃ³Å‚owo
2. SprawdziÄ‡ instrukcjÄ™ na opakowaniu produktu
3. SkontaktowaÄ‡ siÄ™ z naszymi ekspertami

ğŸ“ **Kontakt z ekspertami**: (+48) 14 691 79 79 (pon-pt, 8:00-16:00)
ğŸŒ **Formularz kontaktowy**: https://aquaforest.eu/pl/kontakt/

Nasi specjaliÅ›ci z przyjemnoÅ›ciÄ… odpowiedzÄ… na wszystkie pytania! ğŸŒŠ"""
        
        return {**state, "final_answer": escalation_response}
    
    results = state["search_results"]
    if not results:
        return {
            **state,
            "final_answer": "Nie znalazÅ‚em informacji na ten temat. Czy moÅ¼esz sprecyzowaÄ‡ pytanie?"
        }
    
    # Build context from evaluated results
    context = ""
    product_links = []
    
    for i, result in enumerate(results[:3]):
        context += f"\n--- Å¹RÃ“DÅO {i+1} ---\n"
        context += f"TytuÅ‚: {result['title']}\n"
        context += f"Typ: {result.get('content_type', 'unknown')}\n"
        context += f"TreÅ›Ä‡: {result['full_content']}\n"
        
        if result['content_type'] == 'product' and result['url']:
            product_links.append(f"â€¢ {result['title']}: {result['url']}")
    
    # Add evaluation context
    evaluation_summary = "\n".join(state["evaluation_log"])
    
    answer_prompt = f"""
    {ENHANCED_AQUAFOREST_EXPERT_PROMPT}
    
    === KONTEKST WYSZUKIWANIA ===
    Reasoning attempts: {state['attempt_count']}
    Model confidence achieved: {state['model_confidence']}/10
    Query intent: {state['query_intent']}
    Evaluation history: {evaluation_summary}
    
    === ZWERYFIKOWANE Å¹RÃ“DÅA ===
    {context}
    
    === PYTANIE UÅ»YTKOWNIKA ===
    "{state['original_query']}"
    
    === INSTRUKCJE ===
    1. **TYLKO FACTS** - uÅ¼ywaj wyÅ‚Ä…cznie informacji z kontekstu
    2. **ZERO IMPROVISATION** - nie dodawaj ogÃ³lnej wiedzy
    3. **AQUAFOREST FOCUS** - priorytet dla produktÃ³w AF
    4. **CONVERSATIONAL** - naturalny, przyjazny ton
    5. **PRACTICAL** - konkretne kroki, dawki, parametry
    
    ODPOWIEDÅ¹ EKSPERTA AQUAFOREST:
    """
    
    try:
        response = components.llm.invoke([SystemMessage(content=answer_prompt)])
        answer = response.content
        
        # Add product links if relevant
        if product_links and len(product_links) <= 3:
            answer += f"\n\nğŸ›’ **PRODUKTY AQUAFOREST:**\n" + "\n".join(product_links)
        
        return {**state, "final_answer": answer}
        
    except Exception as e:
        return {**state, "final_answer": f"Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d. SprÃ³buj ponownie."}