# components.py - Core RAG Components
import re
from typing import List, Dict
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.messages import SystemMessage
from config import get_pinecone_index, ENHANCED_AQUAFOREST_EXPERT_PROMPT

class EvaluationRAGComponents:
    def __init__(self):
        self.index = get_pinecone_index()
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)
    
    def search_knowledge(self, query: str, top_k: int = 8) -> List[Dict]:
        """Simple search without score filtering - let model decide quality"""
        try:
            if not self.index:
                return []
            
            query_embedding = self.embeddings.embed_query(query)
            
            results = self.index.query(
                vector=query_embedding,
                namespace='pl',
                top_k=top_k,
                include_metadata=True
            )
            
            # Return ALL results - no score filtering!
            formatted_results = []
            for match in results.get('matches', []):
                metadata = match.get('metadata', {})
                formatted_results.append({
                    'pinecone_score': match.get('score', 0),  # Keep for reference only
                    'title': metadata.get('title', 'Brak tytu≈Çu'),
                    'full_content': metadata.get('full_content', ''),
                    'content_type': metadata.get('content_type', ''),
                    'url': metadata.get('url', ''),
                    'domain': metadata.get('domain', ''),
                    'category': metadata.get('category', ''),
                    'tags': metadata.get('tags', [])
                })
            
            return formatted_results
            
        except Exception as e:
            print(f"‚ùå Search error: {e}")
            return []
    
    def optimize_query_for_attempt(self, original_query: str, attempt: int, previous_evaluations: List[str] = None) -> str:
        """MUCH simpler, natural query optimization"""
        
        if attempt == 1:
            # Keep it simple and natural
            optimization_prompt = f"""
            Skonwertuj pytanie na naturalne polskie s≈Çowa kluczowe dla wyszukiwania:
            
            PYTANIE: "{original_query}"
            
            ZASADY:
            - U≈ºyj naturalnych polskich fraz
            - Zachowaj nazwy produkt√≥w (AF Power Elixir, Component Strong A)
            - Dodaj synonimy (dawkowanie = dawka, stosowanie)
            - MAX 10 s≈Ç√≥w
            
            NATURALNE S≈ÅOWA KLUCZOWE:
            """
            
        elif attempt == 2:
            # Broader terms
            prev_feedback = previous_evaluations[-1] if previous_evaluations else "brak"
            optimization_prompt = f"""
            Rozszerz wyszukiwanie o pokrewne terminy:
            
            ORYGINALNE: "{original_query}"
            POPRZEDNIA PR√ìBA DA≈ÅA: {prev_feedback}
            
            Dodaj synonimy i powiƒÖzane terminy:
            - dawkowanie ‚Üí dawka, stosowanie, dozowanie, aplikacja
            - akwarium ‚Üí zbiornik, pojemno≈õƒá
            - produkty ‚Üí preparaty, ≈õrodki
            
            ROZSZERZONE S≈ÅOWA KLUCZOWE:
            """
            
        else:  # attempt == 3
            # Very broad search
            optimization_prompt = f"""
            Ostatnia pr√≥ba - najszersze wyszukiwanie:
            
            PYTANIE: "{original_query}"
            
            U≈ºyj najog√≥lniejszych termin√≥w z kategorii produktu:
            - morskie, s≈Çodkowodne
            - akwarystyka, hodowla  
            - chemia, preparaty
            - Aquaforest, AF
            
            OG√ìLNE TERMINY:
            """
        
        try:
            response = self.llm.invoke([SystemMessage(content=optimization_prompt)])
            optimized = response.content.strip().replace('"', '')
            
            # Clean up the response - extract only the keywords
            lines = optimized.split('\n')
            for line in lines:
                if line.strip() and not line.startswith('PYTANIE:') and not line.startswith('ZASADY:'):
                    if len(line.strip()) > 5:  # Skip very short lines
                        optimized = line.strip()
                        break
                        
            return optimized
            
        except Exception as e:
            print(f"‚ùå Query optimization error: {e}")
            return original_query
    
    def evaluate_content_quality(self, query: str, results: List[Dict]) -> tuple[float, str]:
        """Model-based evaluation of content quality - NO Pinecone scores used!"""
        
        if not results:
            return 0.0, "No search results to evaluate"
        
        # Prepare context from results (WITHOUT showing Pinecone scores to model)
        context = ""
        for i, result in enumerate(results[:4]):  # Top 4 results
            context += f"\n--- RESULT {i+1} ---\n"
            context += f"Title: {result['title']}\n"
            context += f"Type: {result.get('content_type', 'unknown')}\n"
            context += f"Content: {result['full_content'][:400]}...\n"
        
        evaluation_prompt = f"""
        {ENHANCED_AQUAFOREST_EXPERT_PROMPT}
        
        === EVALUATION TASK ===
        
        USER QUESTION: "{query}"
        
        SEARCH RESULTS TO EVALUATE:
        {context}
        
        === EVALUATION CRITERIA ===
        
        Rate how well these results answer the user's question (1-10):
        
        üéØ **RELEVANCE** (czy tre≈õƒá odpowiada na pytanie):
        - 1-3: Nie odpowiada / zupe≈Çnie inne tematy
        - 4-6: Czƒô≈õciowo zwiƒÖzane ale nie na temat
        - 7-8: Odpowiada na pytanie, dobre informacje
        - 9-10: Perfekcyjnie odpowiada, kompletne info
        
        üìä **COMPLETENESS** (czy wystarczajƒÖce informacje):
        - 1-3: Brak konkret√≥w, og√≥lniki
        - 4-6: Niekt√≥re informacje ale niepe≈Çne
        - 7-8: WystarczajƒÖce do pomocy u≈ºytkownikowi
        - 9-10: Kompletne, wszystko co potrzebne
        
        üîß **ACTIONABILITY** (czy u≈ºytkownik wie co robiƒá):
        - 1-3: Brak praktycznych porad
        - 4-6: Og√≥lne wskaz√≥wki
        - 7-8: Konkretne kroki do wykonania
        - 9-10: Precyzyjne instrukcje krok po krok
        
        RESPOND IN FORMAT:
        CONFIDENCE: [number 1-10]
        REASONING: [brief explanation why this rating]
        """
        
        try:
            response = self.llm.invoke([SystemMessage(content=evaluation_prompt)])
            evaluation_text = response.content
            
            # Extract confidence score
            confidence_match = re.search(r'CONFIDENCE:\s*([1-9]|10)', evaluation_text)
            reasoning_match = re.search(r'REASONING:\s*(.+)', evaluation_text, re.DOTALL)
            
            confidence = float(confidence_match.group(1)) if confidence_match else 5.0
            reasoning = reasoning_match.group(1).strip() if reasoning_match else "No reasoning provided"
            
            return confidence, reasoning
            
        except Exception as e:
            print(f"‚ùå Content evaluation error: {e}")
            return 3.0, f"Evaluation failed: {e}"